{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC5651 - Applied Project - Detecting soccer players in broadcast footage - Notebook\n",
    "\n",
    "This notebook contains the experiments done with Tensorflow 2 and Faster R-CNN for my CSC5651 term project to identify and bound soccer players within live broadcast game footage.\n",
    "\n",
    "## Loading the model\n",
    "\n",
    "Let's start by verfying we're utilizing the GPU hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from IPython.display import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pathlib\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the pre-trained Faster R-CNN model with Inception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract model\n",
    "def download_model(model_name, model_date):\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
    "                                        origin=base_url + model_date + '/' + model_file,\n",
    "                                        untar=True)\n",
    "    return str(model_dir)\n",
    "\n",
    "MODEL_DATE = '20200711'\n",
    "MODEL_NAME = 'faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8'\n",
    "PATH_TO_MODEL_DIR = download_model(MODEL_NAME, MODEL_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jack/.keras/datasets/faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Done! Took 16.93804144859314 seconds\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the labels and create a category index for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_labels(filename):\n",
    "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "    label_dir = tf.keras.utils.get_file(fname=filename,\n",
    "                                        origin=base_url + filename,\n",
    "                                        untar=False)\n",
    "    label_dir = pathlib.Path(label_dir)\n",
    "    return str(label_dir)\n",
    "\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "PATH_TO_LABELS = download_labels(LABEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jack/.keras/datasets/mscoco_label_map.pbtxt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kicking the tires\n",
    "\n",
    "Take the model out for a spin by classifying 10 seconds worth of frames. The video is 25 frames/second, so we'll run object detection on 250 frames, draw bounding boxes on them, restructure them back into a video, and output it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "VIDEO_PATH = '/home/jack/code/csc5651-applied-project/video/Film Role-0 ID-1 T-2 m00s00-000-m00s00-185.avi'\n",
    "OUTPUT_DIR = '/mnt/d/school/csc5651/soc_output/'\n",
    "NUM_SECONDS = 10\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_file = f'{OUTPUT_DIR}output_video.avi'\n",
    "fps = 25\n",
    "frame_size = (1024, 1024)\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "i = 0\n",
    "max_i = 25 * NUM_SECONDS\n",
    "\n",
    "while i < max_i:\n",
    "  ret, frame = cap.read()\n",
    "  resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "\n",
    "  input_tensor = tf.convert_to_tensor(resized_frame)\n",
    "\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "  # input_tensor = np.expand_dims(image_np, 0)\n",
    "  detections = detect_fn(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(detections.pop('num_detections'))\n",
    "  detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "  detections['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "  image = resized_frame.copy()\n",
    "\n",
    "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image,\n",
    "    detections['detection_boxes'],\n",
    "    detections['detection_classes'],\n",
    "    detections['detection_scores'],\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=200,\n",
    "    min_score_thresh=.30,\n",
    "    agnostic_mode=False)\n",
    "\n",
    "  out.write(image)\n",
    "  i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the pre-trained model\n",
    "\n",
    "Now we'll load up the ISSIA dataset with the help of some utility functions borrowed from [FootAndBall](https://github.com/jac99/FootAndBall), a project by Jacek Komorowski et. al. which uses the same dataset for a similar use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames = 3021\n"
     ]
    }
   ],
   "source": [
    "import issia_utils as iu\n",
    "\n",
    "# Start with video 1\n",
    "annotations = iu.read_issia_ground_truth(1, \"data/issia/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `annotations.persons` now contains a dictionary, whose keys are frame numbers and values are lists of tuples containing `(player_id, height, width, x, y)` for each player in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9998', 103, 42, 1385, 320),\n",
       " ('9997', 102, 37, 1, 385),\n",
       " ('9999', 94, 42, 307, 235)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.persons.get(356)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we're only using the player data, so we'll discard the other information for now. If we have time, we'll come back to the ball and test tracking it too. The model we're using has an input size of 1024 x 1024, but the video is a different size. We need to resize the video and the annotations to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = '/home/jack/code/csc5651-applied-project/data/issia/filmrole1.avi'\n",
    "INPUT_WIDTH = 1024\n",
    "INPUT_HEIGHT = 1024\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "width_scale_factor = INPUT_WIDTH / width\n",
    "height_scale_factor = INPUT_HEIGHT / height\n",
    "\n",
    "resized_player_annotations = dict()\n",
    "\n",
    "for k, v in annotations.persons.items():\n",
    "  frame = k\n",
    "  annotation = v\n",
    "  resized_player_annotations[frame] = list()\n",
    "\n",
    "  for player in annotation:\n",
    "    player_id = player[0]\n",
    "    height = int(player[1] * height_scale_factor)\n",
    "    width  = int(player[2] * width_scale_factor)\n",
    "    x = int(player[3] * width_scale_factor)\n",
    "    y = int(player[4] * height_scale_factor)\n",
    "    resized_player_annotations[frame].append((player_id, height, width, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9998', 96, 22, 738, 301),\n",
       " ('9997', 96, 19, 0, 362),\n",
       " ('9999', 88, 22, 163, 221)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_player_annotations.get(356)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resize operation looks pretty sound. Let's verify by resizing some frames from the video and painting on the boundary boxes to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/mnt/d/school/csc5651/soc_output/'\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "output_file = f\"{OUTPUT_DIR}ground_truth_test.avi\"\n",
    "fps = 25\n",
    "frame_size = (1024, 1024)\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "\n",
    "    image = resized_frame.copy()\n",
    "\n",
    "    if i in resized_player_annotations:\n",
    "        for player in resized_player_annotations.get(i):\n",
    "            height = player[1]\n",
    "            width = player[2]\n",
    "            x = player[3]\n",
    "            y = player[4]\n",
    "\n",
    "            cv2.rectangle(image, (x, y), (x + width, y + height), (0, 0, 255), 2)\n",
    "        out.write(image)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing back the output footage, the bounding boxes look approximately accurate, which means the algorithm for rescaling the bounding boxes isn't grossly incorrect. We'll continue with these, and resize and trim each video down to only the frames that have ground-truth data associated with them. The first few seconds of frames in each video are devoid of it, as the camera was calibrating during those. While we're at it, let's also dump out the resized annotations to JSON files for easier reading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n"
     ]
    }
   ],
   "source": [
    "for j in range(1, 7):\n",
    "    cap = cv2.VideoCapture(f'/home/jack/code/csc5651-applied-project/data/issia/filmrole{j}.avi')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    output_file = f'{OUTPUT_DIR}resized_filmrole{j}.avi'\n",
    "    fps = 25\n",
    "    frame_size = (1024, 1024)\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "    annotations = iu.read_issia_ground_truth(1, \"data/issia/\")\n",
    "    for k, v in annotations.persons.items():\n",
    "        frame = k\n",
    "        annotation = v\n",
    "        resized_player_annotations[frame] = list()\n",
    "\n",
    "        for player in annotation:\n",
    "            player_id = player[0]\n",
    "            height = int(player[1] * height_scale_factor)\n",
    "            width = int(player[2] * width_scale_factor)\n",
    "            x = int(player[3] * width_scale_factor)\n",
    "            y = int(player[4] * height_scale_factor)\n",
    "            resized_player_annotations[frame].append((player_id, height, width, x, y))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "        out.write(resized_frame)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}filmrole{j}.json', 'w') as outfile:\n",
    "        json.dump(resized_player_annotations, outfile)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that's done, we can run object detection on one of the resized video clips, and measure the accuracy of the bounding boxes compared to the ground truth. Since we're only looking for persons, we'll filter all the other labels from the detection space of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_convert_detections(detections, label_id, threshold):\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    classes = detections['detection_classes'][0].numpy().astype(int)\n",
    "\n",
    "    mask = (classes == label_id) & (scores > threshold)\n",
    "    scores = scores[mask]\n",
    "    boxes = boxes[mask] * 1024\n",
    "    converted_boxes = list()\n",
    "    for box in boxes:\n",
    "        ymin = box[0]\n",
    "        xmin = box[1]\n",
    "        ymax = box[2]\n",
    "        xmax = box[3]\n",
    "        height = ymax - ymin\n",
    "        width = xmax - xmin\n",
    "        box_x = xmin\n",
    "        box_y = ymin\n",
    "        converted_boxes.append([height, width, box_x, box_y])\n",
    "\n",
    "\n",
    "    return {'detection_scores': scores, 'detection_boxes': np.array(converted_boxes)}\n",
    "\n",
    "INPUT_FILENAME = \"/mnt/d/school/csc5651/soc_output/resized_filmrole1.avi\"\n",
    "ANNOTATION_FILENAME = \"/mnt/d/school/csc5651/soc_output/filmrole1.json\"\n",
    "output_file = f'/mnt/d/school/csc5651/soc_output/dets_and_gt_filmrole1.avi'\n",
    "fps = 25\n",
    "frame_size = (1024, 1024)\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "with open(ANNOTATION_FILENAME) as f:\n",
    "    ground_truth_annotations = json.load(f)\n",
    "\n",
    "ground_truth_annotations = {k: v for k, v in ground_truth_annotations.items() if v}\n",
    "\n",
    "cap = cv2.VideoCapture(INPUT_FILENAME)\n",
    "\n",
    "i = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    i += 1\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if str(i) not in ground_truth_annotations:\n",
    "        continue\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(frame)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "    detections = filter_and_convert_detections(detect_fn(input_tensor), 1, 0.5)\n",
    "\n",
    "    image_with_boxes = frame.copy()\n",
    "    for box in detections['detection_boxes']:\n",
    "        height = int(box[0])\n",
    "        width = int(box[1])\n",
    "        x = int(box[2])\n",
    "        y = int(box[3])\n",
    "        cv2.rectangle(image_with_boxes, (x, y), (x + width, y + height), (0, 0, 255), 2)\n",
    "    \n",
    "    for box in ground_truth_annotations.get(str(i)):\n",
    "        height = box[1]\n",
    "        width = box[2]\n",
    "        x = box[3]\n",
    "        y = box[4]\n",
    "        cv2.rectangle(image_with_boxes, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "    out.write(image_with_boxes)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
