{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC5651 - Applied Project - Detecting soccer players in broadcast footage - Notebook\n",
    "\n",
    "This notebook contains the experiments done with Tensorflow 2 and Faster R-CNN for my CSC5651 term project to identify and bound soccer players within live broadcast game footage.\n",
    "\n",
    "## Loading the model\n",
    "\n",
    "Let's start by verfying we're utilizing the GPU hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 17:17:58.404954: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-30 17:17:58.405028: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-30 17:17:58.405110: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-30 17:17:58.423819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 17:18:00.332591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:00.339200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:00.339232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from IPython.display import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pathlib\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the pre-trained Faster R-CNN model with Inception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract model\n",
    "def download_model(model_name, model_date):\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
    "                                        origin=base_url + model_date + '/' + model_file,\n",
    "                                        untar=True)\n",
    "    return str(model_dir)\n",
    "\n",
    "MODEL_DATE = '20200711'\n",
    "MODEL_NAME = 'faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8'\n",
    "PATH_TO_MODEL_DIR = download_model(MODEL_NAME, MODEL_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jack/.keras/datasets/faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 17:18:06.633314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:06.633379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:06.633396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:07.268808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:07.268854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:07.268860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-30 17:18:07.268911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-30 17:18:07.268953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7365 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Took 11.967283248901367 seconds\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the labels and create a category index for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_labels(filename):\n",
    "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "    label_dir = tf.keras.utils.get_file(fname=filename,\n",
    "                                        origin=base_url + filename,\n",
    "                                        untar=False)\n",
    "    label_dir = pathlib.Path(label_dir)\n",
    "    return str(label_dir)\n",
    "\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "PATH_TO_LABELS = download_labels(LABEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jack/.keras/datasets/mscoco_label_map.pbtxt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kicking the tires\n",
    "\n",
    "Take the model out for a spin by classifying 10 seconds worth of frames. The video is 25 frames/second, so we'll run object detection on 250 frames, draw bounding boxes on them, restructure them back into a video, and output it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "VIDEO_PATH = 'data/issia/filmrole1.avi'\n",
    "OUTPUT_DIR = '/mnt/d/school/csc5651/soc_output/'\n",
    "NUM_SECONDS = 10\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_file = f'{OUTPUT_DIR}output_video.avi'\n",
    "fps = 25\n",
    "frame_size = (1024, 1024)\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "i = 0\n",
    "max_i = 25 * NUM_SECONDS\n",
    "\n",
    "while i < max_i:\n",
    "  ret, frame = cap.read()\n",
    "  resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "\n",
    "  input_tensor = tf.convert_to_tensor(resized_frame)\n",
    "\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "  # input_tensor = np.expand_dims(image_np, 0)\n",
    "  detections = detect_fn(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(detections.pop('num_detections'))\n",
    "  detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "  detections['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "  image = resized_frame.copy()\n",
    "\n",
    "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image,\n",
    "    detections['detection_boxes'],\n",
    "    detections['detection_classes'],\n",
    "    detections['detection_scores'],\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=200,\n",
    "    min_score_thresh=.30,\n",
    "    agnostic_mode=False)\n",
    "\n",
    "  out.write(image)\n",
    "  i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the pre-trained model\n",
    "\n",
    "Now we'll load up the ISSIA dataset with the help of some utility functions borrowed from [FootAndBall](https://github.com/jac99/FootAndBall), a project by Jacek Komorowski et. al. which uses the same dataset for a similar use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames = 3021\n"
     ]
    }
   ],
   "source": [
    "import issia_utils as iu\n",
    "\n",
    "# Start with video 1\n",
    "annotations = iu.read_issia_ground_truth(1, \"data/issia/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `annotations.persons` now contains a dictionary, whose keys are frame numbers and values are lists of tuples containing `(player_id, height, width, x, y)` for each player in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9998', 103, 42, 1385, 320),\n",
       " ('9997', 102, 37, 1, 385),\n",
       " ('9999', 94, 42, 307, 235)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.persons.get(356)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we're only using the player data, so we'll discard the other information for now. If we have time, we'll come back to the ball and test tracking it too. The model we're using has an input size of 1024 x 1024, but the video is a different size. We need to resize the video and the annotations to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = 'data/issia/filmrole1.avi'\n",
    "INPUT_WIDTH = 1024\n",
    "INPUT_HEIGHT = 1024\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "width_scale_factor = INPUT_WIDTH / width\n",
    "height_scale_factor = INPUT_HEIGHT / height\n",
    "\n",
    "resized_player_annotations = dict()\n",
    "\n",
    "for k, v in annotations.persons.items():\n",
    "  frame = k\n",
    "  annotation = v\n",
    "  resized_player_annotations[frame] = list()\n",
    "\n",
    "  for player in annotation:\n",
    "    player_id = player[0]\n",
    "    height = int(player[1] * height_scale_factor)\n",
    "    width  = int(player[2] * width_scale_factor)\n",
    "    x = int(player[3] * width_scale_factor)\n",
    "    y = int(player[4] * height_scale_factor)\n",
    "    resized_player_annotations[frame].append((player_id, height, width, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9998', 96, 22, 738, 301),\n",
       " ('9997', 96, 19, 0, 362),\n",
       " ('9999', 88, 22, 163, 221)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_player_annotations.get(356)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resize operation looks pretty sound. Let's verify by resizing some frames from the video and painting on the boundary boxes to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/mnt/d/school/csc5651/soc_output/'\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "output_file = f\"{OUTPUT_DIR}ground_truth_test.avi\"\n",
    "fps = 25\n",
    "frame_size = (1024, 1024)\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "\n",
    "    image = resized_frame.copy()\n",
    "\n",
    "    if i in resized_player_annotations:\n",
    "        for player in resized_player_annotations.get(i):\n",
    "            height = player[1]\n",
    "            width = player[2]\n",
    "            x = player[3]\n",
    "            y = player[4]\n",
    "\n",
    "            cv2.rectangle(image, (x, y), (x + width, y + height), (0, 0, 255), 2)\n",
    "        out.write(image)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing back the output footage, the bounding boxes look approximately accurate, which means the algorithm for rescaling the bounding boxes isn't grossly incorrect. We'll continue with these, and resize and trim each video down to only the frames that have ground-truth data associated with them. The first few seconds of frames in each video are devoid of it, as the camera was calibrating during those. While we're at it, let's also dump out the resized annotations to JSON files for easier reading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames = 3021\n",
      "Number of frames = 3028\n",
      "Number of frames = 3033\n",
      "Number of frames = 3024\n",
      "Number of frames = 3020\n",
      "Number of frames = 3026\n"
     ]
    }
   ],
   "source": [
    "for j in range(1, 7):\n",
    "    cap = cv2.VideoCapture(f'data/issia/filmrole{j}.avi')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    output_file = f'{OUTPUT_DIR}resized_filmrole{j}.avi'\n",
    "    fps = 25\n",
    "    frame_size = (1024, 1024)\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "    annotations = iu.read_issia_ground_truth(j, \"data/issia/\")\n",
    "    resized_player_annotations = dict()\n",
    "    for k, v in annotations.persons.items():\n",
    "        frame = k\n",
    "        annotation = v\n",
    "        resized_player_annotations[frame] = list()\n",
    "\n",
    "        for player in annotation:\n",
    "            player_id = player[0]\n",
    "            height = int(player[1] * height_scale_factor)\n",
    "            width = int(player[2] * width_scale_factor)\n",
    "            x = int(player[3] * width_scale_factor)\n",
    "            y = int(player[4] * height_scale_factor)\n",
    "            resized_player_annotations[frame].append((player_id, height, width, x, y))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "        out.write(resized_frame)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}filmrole{j}.json', 'w') as outfile:\n",
    "        json.dump(resized_player_annotations, outfile)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that's done, we can run object detection on one of the resized video clips, and measure the accuracy of the bounding boxes compared to the ground truth. Since we're only looking for persons, we'll filter all the other labels from the detection space of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections(detections, label_id, threshold):\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    classes = detections['detection_classes'][0].numpy().astype(int)\n",
    "\n",
    "    mask = (classes == label_id) & (scores > threshold)\n",
    "    scores = scores[mask]\n",
    "    boxes = boxes[mask] * 1024\n",
    "    filtered_boxes = list()\n",
    "    for box in boxes:\n",
    "        filtered_boxes.append(box)\n",
    "\n",
    "    return {'detection_scores': scores, 'detection_boxes': np.array(filtered_boxes)}\n",
    "\n",
    "INPUT_FILENAME = \"/mnt/d/school/csc5651/soc_output/resized_filmrole1.avi\"\n",
    "ANNOTATION_FILENAME = \"/mnt/d/school/csc5651/soc_output/filmrole1.json\"\n",
    "\n",
    "with open(ANNOTATION_FILENAME) as f:\n",
    "    ground_truth_annotations = json.load(f)\n",
    "\n",
    "ground_truth_annotations = {k: v for k, v in ground_truth_annotations.items() if v}\n",
    "\n",
    "cap = cv2.VideoCapture(INPUT_FILENAME)\n",
    "\n",
    "i = 0\n",
    "all_detections = dict()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    i += 1\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if str(i) not in ground_truth_annotations:\n",
    "        continue\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(frame)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "    detections = filter_detections(detect_fn(input_tensor), 1, 0.5)\n",
    "    all_detections[str(i)] = detections\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2 returns detections in the format of `(ymin, xmin, ymax, xmax)`, so we need to convert our ground truth annotations to that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ground_truth_box_to_tf_format(box):\n",
    "    _, height, width, x, y = box\n",
    "    ymin = int(y)\n",
    "    xmin = int(x)\n",
    "    ymax = int(y + height)\n",
    "    xmax = int(x + width)\n",
    "    return np.array([ymin, xmin, ymax, xmax])\n",
    "\n",
    "converted_ground_truth_annotations = dict()\n",
    "for k, v in ground_truth_annotations.items():\n",
    "    converted_boxes = list()\n",
    "    for box in v:\n",
    "        converted_boxes.append(convert_ground_truth_box_to_tf_format(box))\n",
    "    converted_boxes = np.array(converted_boxes)\n",
    "    converted_ground_truth_annotations[k] = {'detection_boxes': converted_boxes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick smoke test and compare the detections with the ground truths for one of the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_scores': array([0.7640616 , 0.66895443, 0.5417378 ], dtype=float32),\n",
       " 'detection_boxes': array([[341.238   ,   0.      , 471.2253  ,  30.268375],\n",
       "        [303.21304 , 739.10297 , 403.62918 , 761.4393  ],\n",
       "        [309.772   ,   0.      , 556.49884 ,  29.684807]], dtype=float32)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_detections['356']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_boxes': array([[301, 738, 397, 760],\n",
       "        [362,   0, 458,  19],\n",
       "        [221, 163, 309, 185]])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_ground_truth_annotations['356']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the ground truths and the detections are in the same format, we'll calculate the accuracy and precision frame-by-frame. This algorithm uses _intersection over union_ to determine hits and misses. That is, the area of overlap between the detected box and the ground truth box, the intersection, as a fraction of the total combined area, the union, of both boxes. 0 means no overlap, and 1 means perfect overlap. For this test, we'll call any detection with an IoU value of >= 0.5 a true positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision: 0.738977517655109\n",
      "Average recall: 0.6425215336495264\n"
     ]
    }
   ],
   "source": [
    "def calculate_iou(gt_box, pred_box):\n",
    "    y1_t, x1_t, y2_t, x2_t = gt_box\n",
    "    y1_p, x1_p, y2_p, x2_p = pred_box\n",
    "\n",
    "    if (x1_p > x2_p) or (y1_p > y2_p):\n",
    "        raise AssertionError(\n",
    "            \"Prediction box is malformed? pred box: {}\".format(pred_box))\n",
    "    if (x1_t > x2_t) or (y1_t > y2_t):\n",
    "        raise AssertionError(\n",
    "            \"Ground Truth box is malformed? true box: {}\".format(gt_box))\n",
    "\n",
    "    if (x2_t < x1_p or x2_p < x1_t or y2_t < y1_p or y2_p < y1_t):\n",
    "        return 0.0\n",
    "\n",
    "    far_x = np.min([x2_t, x2_p])\n",
    "    near_x = np.max([x1_t, x1_p])\n",
    "    far_y = np.min([y2_t, y2_p])\n",
    "    near_y = np.max([y1_t, y1_p])\n",
    "\n",
    "    inter_area = (far_x - near_x + 1) * (far_y - near_y + 1)\n",
    "    true_box_area = (x2_t - x1_t + 1) * (y2_t - y1_t + 1)\n",
    "    pred_box_area = (x2_p - x1_p + 1) * (y2_p - y1_p + 1)\n",
    "    iou = inter_area / (true_box_area + pred_box_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "def calculate_precision_recall(gt_boxes, det_boxes, iou_threshold):\n",
    "    if det_boxes is None:\n",
    "        return 0, 0\n",
    "\n",
    "    # Calculate precision and recall for a set of ground truth and detected bounding boxes\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for det_box in det_boxes:\n",
    "        max_iou = 0\n",
    "        max_gt_box = None\n",
    "\n",
    "        for gt_box in gt_boxes:\n",
    "            iou = calculate_iou(gt_box, det_box)\n",
    "\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "                max_gt_box = gt_box\n",
    "\n",
    "        if max_iou >= iou_threshold:\n",
    "            tp += 1\n",
    "            gt_boxes = [x for x in gt_boxes if not np.array_equal(x, max_gt_box)]\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    fn = len(gt_boxes)\n",
    "\n",
    "    if tp != 0:\n",
    "        precision = tp / float(tp + fp)\n",
    "        recall = tp / float(tp + fn)\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "all_precisions = list()\n",
    "all_recalls = list()\n",
    "for frame_id in converted_ground_truth_annotations.keys():\n",
    "    gt_boxes = converted_ground_truth_annotations.get(frame_id).get('detection_boxes')\n",
    "    det_boxes = all_detections.get(frame_id).get('detection_boxes') if frame_id in all_detections else None\n",
    "    precision, recall = calculate_precision_recall(gt_boxes, det_boxes, 0.5)\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "\n",
    "print(f'Average precision: {np.mean(all_precisions)}')\n",
    "print(f'Average recall: {np.mean(all_recalls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first test is about 74% accurate and showed about 64% recall. Let's now combine this with the other 5 videos and ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILENAME = \"/mnt/d/school/csc5651/soc_output/resized_filmrole{}.avi\"\n",
    "ANNOTATION_FILENAME = \"/mnt/d/school/csc5651/soc_output/filmrole{}.json\"\n",
    "\n",
    "master_ground_truth_annotations = dict()\n",
    "master_all_detections = dict()\n",
    "for i in range(1, 7):\n",
    "    with open(ANNOTATION_FILENAME.format(i)) as f:\n",
    "        ground_truth_annotations = json.load(f)\n",
    "\n",
    "    master_ground_truth_annotations.update({f\"{i}_{k}\": v for k, v in ground_truth_annotations.items() if v})\n",
    "\n",
    "    cap = cv2.VideoCapture(INPUT_FILENAME.format(i))\n",
    "\n",
    "    j = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        j += 1\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if str(j) not in ground_truth_annotations:\n",
    "            continue\n",
    "\n",
    "        input_tensor = tf.convert_to_tensor(frame)\n",
    "        input_tensor = input_tensor[tf.newaxis, ...]\n",
    "        detections = filter_detections(detect_fn(input_tensor), 1, 0.5)\n",
    "        master_all_detections[f\"{i}_{j}\"] = detections\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "converted_master_ground_truth_annotations = dict()\n",
    "for k, v in master_ground_truth_annotations.items():\n",
    "    converted_boxes = list()\n",
    "    for box in v:\n",
    "        converted_boxes.append(convert_ground_truth_box_to_tf_format(box))\n",
    "    converted_boxes = np.array(converted_boxes)\n",
    "    converted_master_ground_truth_annotations[k] = {'detection_boxes': converted_boxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15992"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_master_ground_truth_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15647"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(master_all_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision over frames in all videos: 0.6520690548031569\n",
      "Average recall over frames in all videos: 0.5678544390437077\n"
     ]
    }
   ],
   "source": [
    "all_precisions = list()\n",
    "all_recalls = list()\n",
    "for frame_id in converted_master_ground_truth_annotations.keys():\n",
    "    gt_boxes = converted_master_ground_truth_annotations.get(frame_id).get('detection_boxes')\n",
    "    det_boxes = master_all_detections.get(frame_id).get('detection_boxes') if frame_id in master_all_detections else None\n",
    "    precision, recall = calculate_precision_recall(gt_boxes, det_boxes, 0.5)\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "\n",
    "print(f'Average precision over frames in all videos: {np.mean(all_precisions)}')\n",
    "print(f'Average recall over frames in all videos: {np.mean(all_recalls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/mnt/d/school/csc5651/soc_output/'\n",
    "VIDEO_NUMBER = 3\n",
    "FRAME_NUMBER = 1001\n",
    "INPUT_FILENAME = \"/mnt/d/school/csc5651/soc_output/resized_filmrole{}.avi\".format(VIDEO_NUMBER)\n",
    "\n",
    "cap = cv2.VideoCapture(INPUT_FILENAME)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if i != FRAME_NUMBER:\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "\n",
    "    image = resized_frame.copy()\n",
    "\n",
    "    for box in converted_master_ground_truth_annotations[f'{VIDEO_NUMBER}_{FRAME_NUMBER}'].get('detection_boxes'):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n",
    "\n",
    "    for box in master_all_detections[f'{VIDEO_NUMBER}_{FRAME_NUMBER}'].get('detection_boxes'):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imwrite(f'{OUTPUT_DIR}{VIDEO_NUMBER}_frame_{FRAME_NUMBER}.jpg', image)\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
