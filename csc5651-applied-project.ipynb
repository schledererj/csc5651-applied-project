{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC5651 - Applied Project - Detecting soccer players in broadcast footage - Notebook\n",
    "\n",
    "This notebook contains the experiments done with Tensorflow 2 and Faster R-CNN for my CSC5651 term project to identify and bound soccer players within live broadcast game footage.\n",
    "\n",
    "## Loading the model\n",
    "\n",
    "Let's start by verfying we're utilizing the GPU hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 22:56:16.050190: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-18 22:56:16.050239: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-18 22:56:16.050256: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-18 22:56:16.055096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 22:56:17.506208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:17.509714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:17.509739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the pre-trained Faster R-CNN model with Inception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract model\n",
    "def download_model(model_name, model_date):\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
    "                                        origin=base_url + model_date + '/' + model_file,\n",
    "                                        untar=True)\n",
    "    return str(model_dir)\n",
    "\n",
    "MODEL_DATE = '20200711'\n",
    "MODEL_NAME = 'faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8'\n",
    "PATH_TO_MODEL_DIR = download_model(MODEL_NAME, MODEL_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jack/.keras/datasets/faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 22:56:21.956319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:21.956381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:21.956397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:22.534761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:22.534810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:22.534816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-18 22:56:22.534842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-18 22:56:22.534860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7365 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Took 12.501238346099854 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the labels and create a category index for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def download_labels(filename):\n",
    "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "    label_dir = tf.keras.utils.get_file(fname=filename,\n",
    "                                        origin=base_url + filename,\n",
    "                                        untar=False)\n",
    "    label_dir = pathlib.Path(label_dir)\n",
    "    return str(label_dir)\n",
    "\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "PATH_TO_LABELS = download_labels(LABEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jack/.keras/datasets/mscoco_label_map.pbtxt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kicking the tires\n",
    "\n",
    "Take the model out for a spin by classifying 10 seconds worth of frames. The video is 25 frames/second, so we'll run object detection on 250 frames, draw bounding boxes on them, restructure them back into a video, and output it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "VIDEO_PATH = '/home/jack/code/csc5651-applied-project/video/Film Role-0 ID-1 T-2 m00s00-000-m00s00-185.avi'\n",
    "OUTPUT_DIR = '/mnt/d/school/csc5651/soc_output/'\n",
    "NUM_SECONDS = 10\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_file = f'{OUTPUT_DIR}output_video.avi'\n",
    "fps = 25\n",
    "frame_size = (1024, 1024)\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "i = 0\n",
    "max_i = 25 * NUM_SECONDS\n",
    "\n",
    "while i < max_i:\n",
    "  ret, frame = cap.read()\n",
    "  resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "\n",
    "  input_tensor = tf.convert_to_tensor(resized_frame)\n",
    "\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "  # input_tensor = np.expand_dims(image_np, 0)\n",
    "  detections = detect_fn(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(detections.pop('num_detections'))\n",
    "  detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "  detections['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "  image = resized_frame.copy()\n",
    "\n",
    "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image,\n",
    "    detections['detection_boxes'],\n",
    "    detections['detection_classes'],\n",
    "    detections['detection_scores'],\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=200,\n",
    "    min_score_thresh=.30,\n",
    "    agnostic_mode=False)\n",
    "\n",
    "  out.write(image)\n",
    "  i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the pre-trained model\n",
    "\n",
    "Now we'll load up the ISSIA dataset with the help of some utility functions borrowed from [FootAndBall](https://github.com/jac99/FootAndBall), a project by Jacek Komorowski et. al. which uses the same dataset for a similar use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames = 3021\n"
     ]
    }
   ],
   "source": [
    "import issia_utils as iu\n",
    "\n",
    "# Start with video 1\n",
    "annotations = iu.read_issia_ground_truth(1, \"data/issia/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `annotations.persons` now contains a dictionary, whose keys are frame numbers and values are lists of tuples containing `(player_id, height, width, x, y)` for each player in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('21', 79, 37, 3, 109), ('1', 101, 50, 1298, 287)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.persons.get(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we're only using the player data, so we'll discard the other information for now. If we have time, we'll come back to the ball and test tracking it too. The model we're using has an input size of 1024 x 1024, but the video is a different size. We need to resize the video and the annotations to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = '/home/jack/code/csc5651-applied-project/video/Film Role-0 ID-1 T-2 m00s00-000-m00s00-185.avi'\n",
    "INPUT_WIDTH = 1024\n",
    "INPUT_HEIGHT = 1024\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "width_scale_factor = INPUT_WIDTH / width\n",
    "height_scale_factor = INPUT_HEIGHT / height\n",
    "\n",
    "resized_player_annotations = dict()\n",
    "\n",
    "for k, v in annotations.persons.items():\n",
    "  frame = k\n",
    "  annotation = v\n",
    "  resized_player_annotations[frame] = list()\n",
    "\n",
    "  for player in annotation:\n",
    "    player_id = player[0]\n",
    "    height = int(player[1] * height_scale_factor)\n",
    "    width  = int(player[2] * width_scale_factor)\n",
    "    x = int(player[3] * width_scale_factor)\n",
    "    y = int(player[4] * height_scale_factor)\n",
    "    resized_player_annotations[frame].append((player_id, height, width, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9998', 96, 22, 738, 301),\n",
       " ('9997', 96, 19, 0, 362),\n",
       " ('9999', 88, 22, 163, 221)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_player_annotations.get(356)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resize operation looks pretty sound. Let's verify by resizing some frames from the video and painting on the boundary boxes to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "output_file = f\"{OUTPUT_DIR}ground_truth_test.avi\"\n",
    "fps = 25\n",
    "frame_size = (1024, 1024)\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "\n",
    "    image = resized_frame.copy()\n",
    "\n",
    "    if i in resized_player_annotations:\n",
    "        for player in resized_player_annotations.get(i):\n",
    "            height = player[1]\n",
    "            width = player[2]\n",
    "            x = player[3]\n",
    "            y = player[4]\n",
    "\n",
    "            cv2.rectangle(image, (x, y), (x + width, y + height), (0, 0, 255), 2)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing back the output footage, the bounding boxes look approximately accurate, which means the algorithm for rescaling the bounding boxes isn't grossly incorrect. We'll continue with these, and resize and trim each video down to only the frames that have ground-truth data associated with them. The first few seconds of frames in each video are devoid of it, as the camera was calibrating during those. While we're at it, let's also dump out the resized annotations to JSON files for easier reading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n",
      "Number of frames = 3021\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for j in range(1, 7):\n",
    "    cap = cv2.VideoCapture(f'/home/jack/code/csc5651-applied-project/data/issia/filmrole{j}.avi')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    output_file = f'{OUTPUT_DIR}resized_filmrole{j}.avi'\n",
    "    fps = 25\n",
    "    frame_size = (1024, 1024)\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "    annotations = iu.read_issia_ground_truth(1, \"data/issia/\")\n",
    "    for k, v in annotations.persons.items():\n",
    "        frame = k\n",
    "        annotation = v\n",
    "        resized_player_annotations[frame] = list()\n",
    "\n",
    "        for player in annotation:\n",
    "            player_id = player[0]\n",
    "            height = int(player[1] * height_scale_factor)\n",
    "            width = int(player[2] * width_scale_factor)\n",
    "            x = int(player[3] * width_scale_factor)\n",
    "            y = int(player[4] * height_scale_factor)\n",
    "            resized_player_annotations[frame].append((player_id, height, width, x, y))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "        if i in resized_player_annotations:\n",
    "            out.write(resized_frame)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}filmrole{j}.json', 'w') as outfile:\n",
    "        json.dump(resized_player_annotations, outfile)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
